{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial\n",
    "\n",
    "このハンズオンで行うこと\n",
    "- [Tensorflowの説明](#Tensorflowの説明)\n",
    "- [計算グラフと実行の練習](#計算グラフと実行の練習)\n",
    "- [ロジスティック回帰の実装と実行](#ロジスティック回帰の実装と実行)\n",
    "- [MLPの実装と実行](#MLPの実装と実行)\n",
    "- [学習済みモデルの保存と学習の再開](#学習済みモデルの保存と学習の再開)\n",
    "- [TensorBoardの使用](#TensorBoardの使用)\n",
    "- [精度を上げるためのテクニック](#精度を上げるためのテクニック)\n",
    "- [Tensorflowのさらなる抽象化](#Tensorflowのさらなる抽象化)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Tensorflowの説明\"></a>\n",
    "# 0.　Tensorflowとは？\n",
    "\n",
    "Googleが提供する機械学習用のフレームワーク．\n",
    "機械学習用のフレームワークは他にもたくさん存在するが，Tensorflowは現在世界で最も使用されているフレームワークであると言われている．\n",
    "\n",
    "pythonによって書くが、内部はC++やcudaによって書かれている．\n",
    "\n",
    "'define and run'という形式をとり、まず計算グラフを定義し、それに対してデータを流すという使い方となっている．\n",
    "\n",
    "以下のコードでフレームワークをインストールすることができる．1.2はTensorflowのバージョン．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sudo pip install tensorflow-gpu==1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードで他の必要なライブラリとともに読み込む．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"計算グラフと実行の練習\"></a>\n",
    "# 1.　計算グラフの構築と実行\n",
    "\n",
    "計算グラフを構築するためにTensorflow側が用意している型を用いる必要がある.<br>\n",
    "\n",
    "Tensorflowが用意している種類と使い方は以下のとおり．\n",
    "1. tf.constant ... ハイパーパラメータなど，実行前から形(shape)の決まった定数に用いる．\n",
    "2. tf.placeholder ... データの入力など，実行するまでデータのshapeはわからないが変わらないデータを入れるときに用いる(初期化不要)．例えば，データセットの大きさは実行するまでわからない．\n",
    "3. tf.Variable ... ネットワークの重みなど，学習中に値が変わる最適化対象を入れる(初期化必要)．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0　計算グラフの実行方法\n",
    "計算グラフを構築するだけでは，実際に計算は行われない．<br>\n",
    "計算を実行して値を評価するためには， TensorflowのSessionを作成する必要がある．<br>\n",
    "例えば，$x$という値の出力が欲しい時は，その値をSessionのrunメソッドに渡してあげる．<br>\n",
    "具体的には以下のように書けば良い．\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1　まずはtf.constant(定数)を用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.constant(1)\n",
    "y = tf.constant(2)\n",
    "\n",
    "add_op = tf.add(x, y)\n",
    "print(x,y)\n",
    "print(add_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで表示された結果は定義された計算グラフについての情報で、実際に計算は行われていないことに注意．\n",
    "- 以下のように計算グラフを実行させて値を確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x_, y_, add_op_ = sess.run([x, y, add_op])\n",
    "    print('x is ',x_)\n",
    "    print('y is ', y_)\n",
    "    print('x + y = ',add_op_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "足し算掛け算は以下のようにも書ける．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.constant(1)\n",
    "y = tf.constant(2)\n",
    "\n",
    "## 足し算掛け算は+,*で書いて良い\n",
    "add_op = x+y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x_, y_, add_op_ = sess.run([x, y, add_op])\n",
    "    print('x is ',x_)\n",
    "    print('y is ', y_)\n",
    "    print('x + y = ',add_op_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2　tf.placeholderを用いる(データを流す用)\n",
    "\n",
    "placeholderは初期化不要の変数だが、intかfloatか指定する必要がある．\n",
    "- tf.float32\n",
    "- tf.int32\n",
    "\n",
    "評価対象の変数の計算のために必要なデータの入力はsess.run内のfeed_dict引数内で行うことができる．<br>\n",
    "feed_dictで渡す変数は一つとは限らないので，辞書型で渡す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = tf.placeholder(tf.int32)\n",
    "x = tf.constant(5)\n",
    "op = data*x\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(op, feed_dict={data: 5})\n",
    "    print('5*5=',result)\n",
    "\n",
    "    result = sess.run(op, feed_dict={data: 10})\n",
    "    print('5*10=',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3　tf.Variableを用いる(変数用)\n",
    "\n",
    "- 実行前に全てのVariableは初期化する必要がある．\n",
    "    - sess.run(tf.global_variables_initializer())で一度に初期化できる\n",
    "- Variableへの代入はtf.assignを用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var1 = tf.Variable(0)\n",
    "const1 = tf.constant(2)\n",
    "\n",
    "add_op = var1+const1\n",
    "# Variableへの代入はassignを用いる\n",
    "var1 = tf.assign(var1, add_op)\n",
    "print (var1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    # var1が毎回更新されている\n",
    "    print(sess.run([var1]))\n",
    "    print(sess.run([var1]))\n",
    "    print(sess.run([var1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"ロジスティック回帰の実装と実行\"></a>\n",
    "# 2. ロジスティック回帰\n",
    "\n",
    "0~9の白黒の手書きの数字のデータセットであるMNISTで10クラス分類をロジスティック回帰によって行う．<br>\n",
    "目的関数が最小化されるように学習する．\n",
    "- 目的関数 : softmax cross entropy\n",
    "    - 入力： $Y=(\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\cdots,\\boldsymbol{y}_N)^T\\in \\mathbb{R}^{N\\times K}$, \n",
    "$T=(\\boldsymbol{t}_1,\\boldsymbol{t}_2,\\cdots,\\boldsymbol{t}_N)^T\\in \\mathbb{R}^{N\\times K}$<br />\n",
    "$\\boldsymbol{y}_n$はsoftmax関数の出力，$\\boldsymbol{t}_n$は正解ラベル(1-of-K表現)\n",
    "\n",
    "    - 出力： $c=\\frac{1}{N}\\sum_{n=1}^Ncross\\_entropy(\\boldsymbol{y}_n,\\boldsymbol{t}_n) \n",
    "    = -\\frac{1}{N}\\sum_{n=1}^N \\sum_{i=1}^K t_{k, i} \\log(y_{k, i})\\in\\mathbb{R}^1$\n",
    "$$\n",
    "(\\because cross\\_entropy(\\boldsymbol{y}, \\boldsymbol{t})=-\\sum_i t_i \\log(y_i))\n",
    "$$\n",
    "\n",
    "MNISTデータセットの中身は以下の通り．データセットのサイズは$N$とする．\n",
    "- mnist.data : 画像データ．shapeは$N\\times height \\times width \\times channel$．白黒データなのでチャンネル数$channel=1$. また，画像の高さ$height=28$，幅$width=28$．それぞれのピクセルの値は0~255で表されている．\n",
    "- mnist.target ： 正解ラベル．shapeは$N\\times1$．0~9の値が入っている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0　データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# データのロード\n",
    "mnist = fetch_mldata('MNIST original', data_home='./data/')\n",
    "\n",
    "# data : 画像データ， target : 正解ラベル\n",
    "X, T = mnist.data, mnist.target\n",
    "\n",
    "# 画像データは0~255の数値のなっているので，0~1の値に変換\n",
    "X = X / 255.\n",
    "\n",
    "#　訓練データとテストデータに分ける\n",
    "X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.2)\n",
    "\n",
    "# データのサイズ\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]\n",
    "\n",
    "# ラベルデータをint型に統一し，学習に使いやすいようにone-hot-vectorに変換\n",
    "T_train = np.eye(10)[T_train.astype(\"int\")]\n",
    "T_test = np.eye(10)[T_test.astype(\"int\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot-vectorとは？\n",
    "たとえば$a$が，0~4の整数のみを含むベクトルだとわかっている時に，各行の数字に該当する列の要素のみを1にし，その他を0にする．\n",
    "$$\n",
    "\\begin{equation*}\n",
    "a=\n",
    "\\begin{pmatrix}\n",
    "3\\\\\n",
    "1\\\\\n",
    "4\\\\\n",
    "2\\\\\n",
    "0\n",
    "\\end{pmatrix}\\to\n",
    "a\\_onehot = \n",
    "\\begin{pmatrix}\n",
    "0, 0, 0, 1, 0\\\\\n",
    "0, 1, 0, 0, 0\\\\\n",
    "0, 0, 0, 0, 1\\\\\n",
    "0, 0, 1, 0, 0\\\\\n",
    "1, 0, 0, 0, 0\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "学習における正解ラベルデータは，one-hot-vectorで表されることが多い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('訓練データのサイズは', N_train)\n",
    "print ('テストデータのサイズは', N_test)\n",
    "print ('画像データのshapeは', X_train.shape)\n",
    "print ('ラベルデータのshapeは', T_train.shape)\n",
    "print ('ラベルデータの数値の例：')\n",
    "print (T_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1　ロジスティック回帰クラスの定義\n",
    "LogisticRegressionのcall関数を実装する．\n",
    "\n",
    "入力：$\\boldsymbol{X}\\in\\mathbb{R}^{N\\times n\\_in}$<br>\n",
    "出力：$\\boldsymbol{Y}\\in\\mathbb{R}^{N\\times n\\_out}$\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>ヒント</summary>\n",
    "    <div>\n",
    "    <br>\n",
    "    - $\\boldsymbol{Y}=\\boldsymbol{X} * \\boldsymbol{W} + \\boldsymbol{b}$\n",
    "    <br>\n",
    "    - tf.matmul(a, b) : Tensorflowにおける行列の掛け算$a * b$\n",
    "    </div>\n",
    " </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        # n_in : 入力次元数\n",
    "        # n_out : 出力次元数\n",
    "        self.W = tf.Variable(tf.zeros([n_in, n_out])) # 重み\n",
    "        self.b = tf.Variable(tf.zeros(n_out)) # バイアス\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ### TODO ###\n",
    "        y = tf.matmul(x, self.W) + self.b\n",
    "        ### TODO ###\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2　グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# パラメータ\n",
    "# Learning rate (学習率)\n",
    "lr = 0.7\n",
    "# epoch数 （学習回数）\n",
    "n_epoch = 25\n",
    "# ミニバッチ学習における1バッチのデータ数\n",
    "batchsize = 100\n",
    "\n",
    "# 入力\n",
    "# placeholderを用いると，データのサイズがわからないときにとりあえずNoneとおくことができる．\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28*28次元 \n",
    "t = tf.placeholder(tf.float32, [None, 10]) # 10クラス\n",
    "\n",
    "# モデルの定義\n",
    "# 入力次元数：784，　出力次元数：10\n",
    "model = LogisticRegression(784, 10)\n",
    "\n",
    "# y : predictionの結果\n",
    "y = model(x)\n",
    "\n",
    "# 目的関数:softmax cross entropy\n",
    "# 入力：labels->正解ラベル， logits：predictionの結果\n",
    "# 出力：softmax cross entropyで計算された誤差\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=y)\n",
    "cost = tf.reduce_mean(xentropy)\n",
    "\n",
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "# test用\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3　グラフの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        for i in range(0, N_train, batchsize):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batchsize]]\n",
    "            t_batch = T_train[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x:X_batch, t:t_batch})\n",
    "            sum_loss += loss * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss / N_train\n",
    "        print('Train loss %.3f | ' %(loss), end=\"\")\n",
    "\n",
    "        # Test model\n",
    "        print (\"Accuracy: %.3f\"%(accuracy.eval(feed_dict={x: X_test, t: T_test})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"MLPの実装と実行\"></a>\n",
    "\n",
    "# 3.　MLP\n",
    "\n",
    "多層のニューラルネットワークを学習させる．<br>\n",
    "層を深くすることで表現力が上がり、性能の向上が期待できる．<br>\n",
    "データは先ほどと同じMNISTを用いて比較する．\n",
    "\n",
    "データを読み込みたい場合は，2.0データの準備参照．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0　MLPクラスの定義\n",
    "以下のような要件のネットワークを構築する．\n",
    "```\n",
    "    入力 : x　\n",
    "-> Fully connected layer 1 (input : x, outputの次元数 : 256, 活性化関数 : relu関数)\n",
    "-> Fully connected layer 2 (input : layer1の出力， outputの次元数 : 256, 活性化関数 : relu関数)\n",
    "-> Fully connected layer 3 (input : layer2の出力， outputの次元数 : 10)\n",
    "-> 出力 : out\n",
    "```\n",
    "\n",
    "<details>\n",
    "    <summary>ヒント</summary>\n",
    "    <div><br>\n",
    "    - TensorflowでFully connected layerはtf.layers.dense (inputs, units, activation=None)で呼ぶことができる．\n",
    "    <br>\n",
    "    - inputs : 入力データ\n",
    "    <br>\n",
    "    - units :  outputの次元数\n",
    "    <br>\n",
    "    - activation : 活性化関数の種類（デフォルトでは無し）\n",
    "    <br>\n",
    "    - relu関数はTensorflowでtf.nn.reluと表される．\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(x):\n",
    "    ### TODO\n",
    "    layer_1 = tf.layers.dense(x, 256, activation=tf.nn.relu)\n",
    "    layer_2 = tf.layers.dense(layer_1, 256, activation=tf.nn.relu)\n",
    "    out = tf.layers.dense(layer_2, 10)\n",
    "    ### TODO\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1　グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# パラメータ\n",
    "# Learning rate (学習率)\n",
    "lr = 0.1\n",
    "# epoch数 （学習回数）\n",
    "n_epoch = 25\n",
    "# ミニバッチ学習における1バッチのデータ数\n",
    "batchsize = 100\n",
    "\n",
    "# 入力\n",
    "# placeholderを用いると，データのサイズがわからないときにとりあえずNoneとおくことができる．\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28*28次元 \n",
    "t = tf.placeholder(tf.float32, [None, 10]) # 10クラス\n",
    "\n",
    "# MLPクラスのモデルを用いてpredictionを行う\n",
    "y = MLP(x)\n",
    "\n",
    "# 目的関数:softmax cross entropy\n",
    "# 入力：labels->正解ラベル， logits：predictionの結果\n",
    "# 出力：softmax cross entropyで計算された誤差\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=y)\n",
    "cost = tf.reduce_mean(xentropy)\n",
    "\n",
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "# test用\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2　グラフの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        for i in range(0, N_train, batchsize):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batchsize]]\n",
    "            t_batch = T_train[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x:X_batch, t:t_batch})\n",
    "            sum_loss += loss * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss / N_train\n",
    "        print('Train loss %.5f | ' %(loss), end=\"\")\n",
    "\n",
    "        # Test model\n",
    "        print (\"Test Accuracy: %.3f\"%(accuracy.eval(feed_dict={x: X_test, t: T_test})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"学習済みモデルの保存と学習の再開\"></a>\n",
    "# 4.　モデルの保存と再利用\n",
    "\n",
    "- tf.train.Saver()を用いる\n",
    "\n",
    "ここでは，先ほどグラフを構築したMLPをそのまま用いる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0　モデルを保存するディレクトリを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# モデルの保存先ディレクトリのpath\n",
    "model_path = \"./model/\"\n",
    "\n",
    "# もし該当のディレクトリが存在しなかったらディレクトリを作成する\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1　モデルを保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.train.Saverのコンストラクタ\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        for i in range(0, N_train, batchsize):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batchsize]]\n",
    "            t_batch = T_train[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x:X_batch, t:t_batch})\n",
    "            sum_loss += loss * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss / N_train\n",
    "        print('Train loss %.5f | ' %(loss), end=\"\")\n",
    "\n",
    "        # Test model\n",
    "        print (\"Test Accuracy: %.3f\"%(accuracy.eval(feed_dict={x: X_test, t: T_test})))\n",
    "        \n",
    "        ###　モデルの保存\n",
    "        saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2　上で保存したモデルを読み込み，学習を再開する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #　model_pathで指定されたモデルを読み込む\n",
    "    saver.restore(sess, model_path)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "        \n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        for i in range(0, N_train, batchsize):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batchsize]]\n",
    "            t_batch = T_train[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x:X_batch, t:t_batch})\n",
    "            sum_loss += loss * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss / N_train\n",
    "        print('Train loss %.5f | ' %(loss), end=\"\")\n",
    "\n",
    "        # Test model\n",
    "        print (\"Test Accuracy: %.3f\"%(accuracy.eval(feed_dict={x: X_test, t: T_test})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"TensorBoardの使用\"></a>\n",
    "# 5. TensorBoardを用いて学習経過を可視化する\n",
    "\n",
    "TensorBoardとは，Tensorflowが提供する可視化ツール．<br>\n",
    "学習経過やグラフ構造をブラウザ上で動的に確認できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0　logを保存する用のディレクトリを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# logの保存先ディレクトリのpath\n",
    "logs_path = './log/'\n",
    "\n",
    "# もし該当のディレクトリが存在したら削除する\n",
    "if os.path.exists(logs_path):\n",
    "    shutil.rmtree(logs_path)\n",
    "    \n",
    "# 保存先ディレクトリを作成する\n",
    "os.mkdir(logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1　グラフを構築する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# パラメータ\n",
    "# Learning rate (学習率)\n",
    "lr = 0.001\n",
    "# epoch数 （学習回数）\n",
    "n_epoch = 25\n",
    "# ミニバッチ学習における1バッチのデータ数\n",
    "batchsize = 100\n",
    "\n",
    "# 入力\n",
    "# placeholderを用いると，データのサイズがわからないときにとりあえずNoneとおくことができる．\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28*28次元 \n",
    "t = tf.placeholder(tf.float32, [None, 10]) # 10クラス\n",
    "\n",
    "# MLPクラスのモデルを用いてpredictionを行う\n",
    "y = MLP(x)\n",
    "\n",
    "# 目的関数:softmax cross entropy\n",
    "# 入力：labels->正解ラベル， logits：predictionの結果\n",
    "# 出力：softmax cross entropyで計算された誤差\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=y)\n",
    "cost = tf.reduce_mean(xentropy)\n",
    "\n",
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "# test用\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 可視化対象に誤差を設定する\n",
    "tf.summary.scalar(\"train_loss\", cost)\n",
    "# 可視化対象に正解率を設定する\n",
    "tf.summary.scalar(\"train_accuracy\", accuracy)\n",
    "# 全ての可視化対象を統合して一つの操作にまとめる\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2　グラフを実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        for i in range(0, N_train, batchsize):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batchsize]]\n",
    "            t_batch = T_train[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss, summary = sess.run([optimizer, cost, merged_summary_op], feed_dict={x:X_batch, t:t_batch})\n",
    "            summary_writer.add_summary(summary, epoch * N_train + len(perm[:i+batchsize]))\n",
    "            sum_loss += loss * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss / N_train\n",
    "        print('Train loss %.5f | ' %(loss), end=\"\")\n",
    "\n",
    "        # Test model\n",
    "        print (\"Test Accuracy: %.3f\"%(accuracy.eval(feed_dict={x: X_test, t: T_test})))\n",
    "\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.3　TensorBoardを立ち上げる\n",
    "上記の学習を始めたら、terminal上で以下を実行\n",
    "- tensorboard --logdir ./log/\n",
    "(logのディレクトリを指定) \n",
    "- ブラウザ上でnotebookのhttp://{サーバーのアドレス}:6006/ にアクセスする．\n",
    "- サーバーのアドレスとは，現在用いているjupyter notebookのURLのうち，\"ec2-....compute.amazonaws.com\"で表される形式の文字列．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='精度を上げるためのテクニック'></a>\n",
    "# 6. 精度を上げるためのテクニック\n",
    "\n",
    "初期設定では，ロジスティック回帰，MLPともに98%前後のaccuracyになったのではないかと思います．\n",
    "\n",
    "こうした学習においては，以下のような重みの初期値，Dropout，data augmentationなどのテクニックを用いて新たなモデルを作成することで，さらに性能が向上する可能性があります．\n",
    "\n",
    "モデルを工夫して，よりよい性能を発揮するネットワークを構築してみましょう．\n",
    "\n",
    "\n",
    "## 6.0 パラメータの初期化\n",
    "\n",
    "### Heの初期化\n",
    "```python\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden\")\n",
    "```\n",
    "\n",
    "### Xavierの初期化\n",
    "```python\n",
    "xavier_init = tf.glorot_uniform_initializer()\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.sigmoid, kernel_initializer=xavier_init, name=\"hidden\")\n",
    "```\n",
    "\n",
    "## 6.1 バッチ正規化\n",
    "\n",
    "```python\n",
    "hidden = tf.layers.dense(X, n_hidden, name=\"hidden\")\n",
    "bn = tf.layers.batch_normalization(hidden, training=training, momentum=0.9)\n",
    "bn_act = tf.nn.relu(bn)\n",
    "```\n",
    "\n",
    "\n",
    "## 6.2 最適化手法\n",
    "\n",
    "\n",
    "### Momentum\n",
    "```python\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "```\n",
    "\n",
    "### AdaGrad\n",
    "```python\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "```\n",
    "\n",
    "### Adam\n",
    "```python\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "```\n",
    "\n",
    "\n",
    "## 6.3 過学習防ぐための正則化\n",
    "\n",
    "### Weight Decay\n",
    "loss関数にネットワークの重みを加えることで，パラメータが大きくなりすぎることを防ぐ．\n",
    "```python\n",
    "L2_sqr = tf.nn.l2_loss(W)\n",
    "lambda_2 = 0.01\n",
    "loss = cross_entropy + lambda_2 * L2_sqr\n",
    "```\n",
    "\n",
    "### Dropout\n",
    "```python\n",
    "dropout_rate = 0.5\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, name=\"hidden\")\n",
    "hidden_drop = tf.layers.dropout(hidden, dropout_rate, training=training)\n",
    "```\n",
    "\n",
    "\n",
    "## 6.4 Data Augmentation（データ拡張）\n",
    "\n",
    "- 拡大縮小\n",
    "- 回転\n",
    "- 反転\n",
    "- ノイズ追加\n",
    "- クロップ\n",
    "などなど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='Tensorflowのさらなる抽象化'></a>\n",
    "# < Tensorflow 応用編 >\n",
    "\n",
    "Tensorflowには学習コードをさらに抽象化する<strong>Estimator</strong>というものがある．<br>\n",
    "Estimatorを用いるとEstimator.train()を呼ぶだけでepoch数だけfor文を書くなどの手間が省けるため，コードの可読性が上がり，<br>\n",
    "実用でも広く使われている．\n",
    "\n",
    "## Estimatorの使い方\n",
    "Estimatorの使用方法について大まかなコードの流れの例を説明したものが以下のサンプルコードである．\n",
    "```python\n",
    "estimator = tf.estimator.Estimator(model_fn)\n",
    "estimator.train(train_input_fn)\n",
    "result = estimator.evaluate(test_input_fn)\n",
    "```\n",
    "model_fnにモデルの内容（ネットワーク構造，ロス関数，optimizerの設定）を書き，Estimatorに渡す．<br>\n",
    "Estimatorは訓練用のデータの情報が入っているtrain_input_fnを引数としてtrainメソッドを呼ぶことで訓練を行う．<br>\n",
    "さらに，テスト用のデータの情報が入っているtest_input_fnを引数としてevaluateメソッドを呼ぶことでテストを行い，学習データになかったデータに対する汎用性を評価する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimatorでの実装時に用いられる関数はone-hot-vectorではなくクラスラベルをそのまま入力することが多い\n",
    "t_train = np.argmax(T_train, axis=1)\n",
    "t_test = np.argmax(T_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    # モデルを定義する\n",
    "    net = features['x']\n",
    "    for units in params['hidden_units']:\n",
    "        # 入力がnetで出力がunits個，活性化関数がrelu関数である全結合層\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "\n",
    "    # netを入力としてクラス分のunit数に出力することでlogitsを計算する\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "    \n",
    "    # ロスの定義\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # 推定結果を計算する\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    \n",
    "    # 正解率の計算方法を指定する\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predicted_classes)\n",
    "    metrics = {'accuracy': accuracy}\n",
    "  \n",
    "    # evaluationモードのとき\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # lossを最小化するオプティマイザを定義する\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "     \n",
    "    # trainモードのとき\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    batch_size = 100\n",
    "\n",
    "    # 訓練データ用\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_train},\n",
    "        y=t_train,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=5,\n",
    "        shuffle=True)\n",
    "    \n",
    "    # テストデータ用\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_test},\n",
    "        y=t_test,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "    # model functionを用いてestimatorを定義する\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=my_model,\n",
    "        params={\n",
    "            'hidden_units': [256, 256], #  256ノード持つ2つの隠れ層\n",
    "            'n_classes': 10, # モデルは結果を3つのクラスから選ぶ\n",
    "        })\n",
    "     \n",
    "    # モデルを訓練\n",
    "    estimator.train(input_fn=train_input_fn)\n",
    "    \n",
    "    # モデルの評価\n",
    "    eval_result = estimator.evaluate(input_fn=eval_input_fn)\n",
    "    \n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
